{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # HoVerNet - The CoNIC Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this notebook\n",
    "In this notebook, we provide a pretrained\n",
    "[HoVerNet](https://arxiv.org/abs/1812.06499)\n",
    "as the baseline model for the CoNIC training data.\n",
    "The HoVerNet utilized here was trained using\n",
    "only data from the CoNIC training set. You can download the pretrained\n",
    "weights [here](https://drive.google.com/file/d/1oVCD4_kOS-8Wu-eS5ZqzE30F9V3Id78d/view?usp=sharing).\n",
    "\n",
    "In the making of the baseline model, we perform following steps:\n",
    "- Generating training and validation split.\n",
    "- Perform the inference to get raw output.\n",
    "- Perform the post-processing to convert the output into an approriate form\n",
    "for the evaluation process in `compute_stats.py`.\n",
    "\n",
    "All in all, the HoVerNet trained on the data split obtained\n",
    "from this notebook achieved the validation results as follows:\n",
    "\n",
    "**Nucleus Instance Segmentation and Classification**\n",
    "---------------------------\n",
    "| PQ     | mPQ<sup>+</sup>|\n",
    "|--------|----------------|\n",
    "| 0.6149 | 0.4998         |\n",
    "---------------------------\n",
    "\n",
    "\n",
    "**Cell Composition**\n",
    "-----------------\n",
    "| R<sup>2</sup> |\n",
    "|---------------|\n",
    "| 0.8585        |\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Importing Libraries and Workspace Settings\n",
    " We import some Python modules that are utilised throughout the notebook.\n",
    "\n",
    " > **Note**: We use `tiatoolbox` extensively in this repository.\n",
    " > You can refer to [here](https://github.com/TissueImageAnalytics/tiatoolbox) for installation instructions.\n",
    " > For this notebook, we use the `develop` branch rather than `master`.\n",
    "\n",
    " We also declare the folders which contain the input\n",
    " data and the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Console\n",
    "# !apt-get -y install libopenjp2-7-dev libopenjp2-tools openslide-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tiatoolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\Conic_Challenge\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\envs\\Conic_Challenge\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\envs\\Conic_Challenge\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "def remap_label(pred, by_size=False):\n",
    "    \"\"\"Rename all instance id so that the id is contiguous i.e [0, 1, 2, 3] \n",
    "    not [0, 2, 4, 6]. The ordering of instances (which one comes first) \n",
    "    is preserved unless by_size=True, then the instances will be reordered\n",
    "    so that bigger nucler has smaller ID.\n",
    "\n",
    "    Args:\n",
    "        pred (ndarray): the 2d array contain instances where each instances is marked\n",
    "            by non-zero integer.\n",
    "        by_size (bool): renaming such that larger nuclei have a smaller id (on-top).\n",
    "\n",
    "    Returns:\n",
    "        new_pred (ndarray): Array with continguous ordering of instances.\n",
    "\n",
    "    \"\"\"\n",
    "    pred_id = list(np.unique(pred))\n",
    "    pred_id.remove(0)\n",
    "    if len(pred_id) == 0:\n",
    "        return pred  # no label\n",
    "    if by_size:\n",
    "        pred_size = []\n",
    "        for inst_id in pred_id:\n",
    "            size = (pred == inst_id).sum()\n",
    "            pred_size.append(size)\n",
    "        # sort the id by size in descending order\n",
    "        pair_list = zip(pred_id, pred_size)\n",
    "        pair_list = sorted(pair_list, key=lambda x: x[1], reverse=True)\n",
    "        pred_id, pred_size = zip(*pair_list)\n",
    "\n",
    "    new_pred = np.zeros(pred.shape, np.int32)\n",
    "    for idx, inst_id in enumerate(pred_id):\n",
    "        new_pred[pred == inst_id] = idx + 1\n",
    "    return new_pred\n",
    "\n",
    "\n",
    "def cropping_center(x, crop_shape, batch=False):\n",
    "    \"\"\"Crop an array at the centre with specified dimensions.\"\"\"\n",
    "    orig_shape = x.shape\n",
    "    if not batch:\n",
    "        h0 = int((orig_shape[0] - crop_shape[0]) * 0.5)\n",
    "        w0 = int((orig_shape[1] - crop_shape[1]) * 0.5)\n",
    "        x = x[h0 : h0 + crop_shape[0], w0 : w0 + crop_shape[1]]\n",
    "    else:\n",
    "        h0 = int((orig_shape[1] - crop_shape[0]) * 0.5)\n",
    "        w0 = int((orig_shape[2] - crop_shape[1]) * 0.5)\n",
    "        x = x[:, h0 : h0 + crop_shape[0], w0 : w0 + crop_shape[1]]\n",
    "    return x\n",
    "\n",
    "\n",
    "def rm_n_mkdir(dir_path):\n",
    "    \"\"\"Remove and make directory.\"\"\"\n",
    "    if os.path.isdir(dir_path):\n",
    "        shutil.rmtree(dir_path)\n",
    "    os.makedirs(dir_path)\n",
    "\n",
    "\n",
    "def rmdir(dir_path):\n",
    "    if os.path.isdir(dir_path):\n",
    "        shutil.rmtree(dir_path)\n",
    "    return\n",
    "\n",
    "\n",
    "def recur_find_ext(root_dir, ext_list):\n",
    "    \"\"\"Recursively find all files in directories end with the `ext` such as `ext='.png'`.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): Root directory to grab filepaths from.\n",
    "        ext_list (list): File extensions to consider.\n",
    "\n",
    "    Returns:\n",
    "        file_path_list (list): sorted list of filepaths.\n",
    "    \"\"\"\n",
    "    file_path_list = []\n",
    "    for cur_path, dir_list, file_list in os.walk(root_dir):\n",
    "        for file_name in file_list:\n",
    "            file_ext = pathlib.Path(file_name).suffix\n",
    "            if file_ext in ext_list:\n",
    "                full_path = os.path.join(cur_path, file_name)\n",
    "                file_path_list.append(full_path)\n",
    "    file_path_list.sort()\n",
    "    return file_path_list\n",
    "\n",
    "\n",
    "def rm_n_mkdir(dir_path):\n",
    "    \"\"\"Remove and then make a new directory.\"\"\"\n",
    "    if os.path.isdir(dir_path):\n",
    "        shutil.rmtree(dir_path)\n",
    "    os.makedirs(dir_path)\n",
    "\n",
    "\n",
    "def get_bounding_box(img):\n",
    "    \"\"\"Get the bounding box coordinates of a binary input- assumes a single object.\n",
    "\n",
    "    Args:\n",
    "        img: input binary image.\n",
    "\n",
    "    Returns:\n",
    "        bounding box coordinates\n",
    "\n",
    "    \"\"\"\n",
    "    rows = np.any(img, axis=1)\n",
    "    cols = np.any(img, axis=0)\n",
    "    rmin, rmax = np.where(rows)[0][[0, -1]]\n",
    "    cmin, cmax = np.where(cols)[0][[0, -1]]\n",
    "    # due to python indexing, need to add 1 to max\n",
    "    # else accessing will be 1px in the box, not out\n",
    "    rmax += 1\n",
    "    cmax += 1\n",
    "    return [rmin, rmax, cmin, cmax]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# from .utils import cropping_center\n",
    "\n",
    "\n",
    "class PatchExtractor(object):\n",
    "    \"\"\"Extractor to generate patches with or without padding.\n",
    "    Turn on debug mode to see how it is done.\n",
    "\n",
    "    Args:\n",
    "        x: input image, should be of shape HWC\n",
    "        win_size: a tuple of (h, w).\n",
    "        step_size: a tuple of (h, w).\n",
    "        debug: flag to see how it is done.\n",
    "        \n",
    "    Returns:\n",
    "        a list of sub patches, each patch has dtype same as x.\n",
    "\n",
    "    Examples:\n",
    "        >>> xtractor = PatchExtractor(450, 120)\n",
    "        >>> img = np.full([1200, 1200, 3], 255, np.uint8)\n",
    "        >>> patches = xtractor.extract(img, 'mirror')\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, win_size, step_size, debug=False):\n",
    "        assert isinstance(win_size, int)\n",
    "        assert isinstance(step_size, int)\n",
    "\n",
    "        self.patch_type = \"mirror\"\n",
    "        self.win_size = [win_size, win_size]\n",
    "        self.step_size = [step_size, step_size]\n",
    "        self.debug = debug\n",
    "        self.counter = 0\n",
    "\n",
    "    def __get_patch(self, x, ptx):\n",
    "        pty = (ptx[0] + self.win_size[0], ptx[1] + self.win_size[1])\n",
    "        win = x[ptx[0] : pty[0], ptx[1] : pty[1]]\n",
    "        assert (\n",
    "            win.shape[0] == self.win_size[0] and win.shape[1] == self.win_size[1]\n",
    "        ), \"[BUG] Incorrect Patch Size {0}\".format(win.shape)\n",
    "        if self.debug:\n",
    "            if self.patch_type == \"mirror\":\n",
    "                cen = cropping_center(win, self.step_size)\n",
    "                cen = cen[..., self.counter % 3]\n",
    "                cen.fill(150)\n",
    "            cv2.rectangle(x, ptx, pty, (255, 0, 0), 2)\n",
    "            plt.imshow(x)\n",
    "            plt.show(block=False)\n",
    "            plt.pause(1)\n",
    "            plt.close()\n",
    "            self.counter += 1\n",
    "        return win\n",
    "\n",
    "    def __extract_valid(self, x):\n",
    "        \"\"\"Extracted patches without padding, only work in case win_size > step_size.\n",
    "        \n",
    "        Note: to deal with the remaining portions which are at the boundary a.k.a\n",
    "        those which do not fit when slide left->right, top->bottom), we flip \n",
    "        the sliding direction then extract 1 patch starting from right / bottom edge. \n",
    "        There will be 1 additional patch extracted at the bottom-right corner.\n",
    "\n",
    "        Args:\n",
    "            x: input image, should be of shape HWC.\n",
    "            win_size: a tuple of (h, w).\n",
    "            step_size: a tuple of (h, w).\n",
    "            \n",
    "        Returns:\n",
    "            a list of sub patches, each patch is same dtype as x.\n",
    "\n",
    "        \"\"\"\n",
    "        im_h = x.shape[0]\n",
    "        im_w = x.shape[1]\n",
    "\n",
    "        def extract_infos(length, win_size, step_size):\n",
    "            flag = (length - win_size) % step_size != 0\n",
    "            last_step = math.floor((length - win_size) / step_size)\n",
    "            last_step = (last_step + 1) * step_size\n",
    "            return flag, last_step\n",
    "\n",
    "        h_flag, h_last = extract_infos(im_h, self.win_size[0], self.step_size[0])\n",
    "        w_flag, w_last = extract_infos(im_w, self.win_size[1], self.step_size[1])\n",
    "\n",
    "        sub_patches = []\n",
    "        #### Deal with valid block\n",
    "        for row in range(0, h_last, self.step_size[0]):\n",
    "            for col in range(0, w_last, self.step_size[1]):\n",
    "                win = self.__get_patch(x, (row, col))\n",
    "                sub_patches.append(win)\n",
    "        #### Deal with edge case\n",
    "        if h_flag:\n",
    "            row = im_h - self.win_size[0]\n",
    "            for col in range(0, w_last, self.step_size[1]):\n",
    "                win = self.__get_patch(x, (row, col))\n",
    "                sub_patches.append(win)\n",
    "        if w_flag:\n",
    "            col = im_w - self.win_size[1]\n",
    "            for row in range(0, h_last, self.step_size[0]):\n",
    "                win = self.__get_patch(x, (row, col))\n",
    "                sub_patches.append(win)\n",
    "        if h_flag and w_flag:\n",
    "            ptx = (im_h - self.win_size[0], im_w - self.win_size[1])\n",
    "            win = self.__get_patch(x, ptx)\n",
    "            sub_patches.append(win)\n",
    "        return sub_patches\n",
    "\n",
    "    def __extract_mirror(self, x):\n",
    "        \"\"\"Extracted patches with mirror padding the boundary such that the \n",
    "        central region of each patch is always within the orginal (non-padded)\n",
    "        image while all patches' central region cover the whole orginal image.\n",
    "\n",
    "        Args:\n",
    "            x: input image, should be of shape HWC.\n",
    "            win_size: a tuple of (h, w).\n",
    "            step_size: a tuple of (h, w).\n",
    "            \n",
    "        Returns:\n",
    "            a list of sub patches, each patch is same dtype as x.\n",
    "            \n",
    "        \"\"\"\n",
    "        diff_h = self.win_size[0] - self.step_size[0]\n",
    "        padt = diff_h // 2\n",
    "        padb = diff_h - padt\n",
    "\n",
    "        diff_w = self.win_size[1] - self.step_size[1]\n",
    "        padl = diff_w // 2\n",
    "        padr = diff_w - padl\n",
    "\n",
    "        pad_type = \"constant\" if self.debug else \"reflect\"\n",
    "        x = np.lib.pad(x, ((padt, padb), (padl, padr), (0, 0)), pad_type)\n",
    "        sub_patches = self.__extract_valid(x)\n",
    "        return sub_patches\n",
    "\n",
    "    def extract(self, x, patch_type):\n",
    "        \"\"\"Extract the patches.\n",
    "        \n",
    "        Args:\n",
    "            x: input array to extract patches from.\n",
    "            patch_type:\n",
    "                'valid' extract patches without reflection at boundary. However, \n",
    "                    in case the img size < patch size, the img will be mirror-padded \n",
    "                    to fit a single patch.\n",
    "                'mirror' extract patches with mirror padding at boundary.\n",
    "\n",
    "        \"\"\"\n",
    "        patch_type = patch_type.lower()\n",
    "        self.patch_type = patch_type\n",
    "        if patch_type == \"valid\":\n",
    "            # padding in case request size larger img size\n",
    "            img_y, img_x = x.shape[:2]\n",
    "            if img_x < self.win_size[1]:\n",
    "                diff_x = self.win_size[1] - img_x\n",
    "                pad_x = int(math.ceil(diff_x / 2))\n",
    "                x = np.lib.pad(x, ((0, 0), (pad_x, pad_x), (0, 0)), \"reflect\")\n",
    "            if img_y < self.win_size[0]:\n",
    "                diff_y = self.win_size[0] - img_y\n",
    "                pad_y = int(math.ceil(diff_y / 2))\n",
    "                x = np.lib.pad(x, ((pad_y, pad_y), (0, 0), (0, 0)), \"reflect\")\n",
    "            return self.__extract_valid(x)\n",
    "        elif patch_type == \"mirror\":\n",
    "            return self.__extract_mirror(x)\n",
    "        else:\n",
    "            assert False, \"Unknown Patch Type [%s]\" % patch_type\n",
    "        return\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # example for debug\n",
    "#     xtractor = PatchExtractor((256, 256), (128, 128), debug=True)\n",
    "#     a = np.full([1200, 1200, 3], 255, np.uint8)\n",
    "#     xtractor.extract(a, \"mirror\")\n",
    "#     xtractor.extract(a, \"valid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path can also be read from a config file, etc.\n",
    "OPENSLIDE_PATH = r'C:\\openslide -win64-20220811\\bin' # Note that if we change the version of this or move the directory we have to change this path\n",
    "\n",
    "import os\n",
    "if hasattr(os, 'add_dll_directory'):\n",
    "    # Python >= 3.8 on Windows\n",
    "    with os.add_dll_directory(OPENSLIDE_PATH):\n",
    "        import openslide\n",
    "else:\n",
    "    import openslide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "|2022-08-30|15:45:24.638| [INFO] Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "|2022-08-30|15:45:24.639| [INFO] NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from collections import OrderedDict\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models.resnet import Bottleneck as ResNetBottleneck\n",
    "from torchvision.models.resnet import ResNet\n",
    "\n",
    "sys.path.append(\"../../tiatoolbox\")\n",
    "\n",
    "from tiatoolbox.models.abc import ModelABC\n",
    "from tiatoolbox.models.architecture.hovernet import HoVerNet as TIAHoVerNet\n",
    "from tiatoolbox.models.architecture.utils import UpSample2x\n",
    "\n",
    "\n",
    "class ResNetExt(ResNet):\n",
    "    def _forward_impl(self, x, freeze):\n",
    "        # See note [TorchScript super()]\n",
    "        if self.training:\n",
    "            x = self.conv1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.relu(x)\n",
    "            with torch.set_grad_enabled(not freeze):\n",
    "                x1 = x = self.layer1(x)\n",
    "                x2 = x = self.layer2(x)\n",
    "                x3 = x = self.layer3(x)\n",
    "                x4 = x = self.layer4(x)\n",
    "        else:\n",
    "            x = self.conv1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.relu(x)\n",
    "            x1 = x = self.layer1(x)\n",
    "            x2 = x = self.layer2(x)\n",
    "            x3 = x = self.layer3(x)\n",
    "            x4 = x = self.layer4(x)\n",
    "        return x1, x2, x3, x4\n",
    "\n",
    "    def forward(self, x: torch.Tensor, freeze: bool = False) -> torch.Tensor:\n",
    "        return self._forward_impl(x, freeze)\n",
    "\n",
    "    @staticmethod\n",
    "    def resnet50(num_input_channels, pretrained=None):\n",
    "        model = ResNetExt(ResNetBottleneck, [3, 4, 6, 3])\n",
    "        model.conv1 = nn.Conv2d(\n",
    "            num_input_channels, 64, 7, stride=1, padding=3)\n",
    "        if pretrained is not None:\n",
    "            pretrained = torch.load(pretrained)\n",
    "            (\n",
    "                missing_keys, unexpected_keys\n",
    "            ) = model.load_state_dict(pretrained, strict=False)\n",
    "        return model\n",
    "\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    \"\"\"Dense Block as defined in:\n",
    "\n",
    "    Huang, Gao, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q. Weinberger. \n",
    "    \"Densely connected convolutional networks.\" In Proceedings of the IEEE conference \n",
    "    on computer vision and pattern recognition, pp. 4700-4708. 2017.\n",
    "\n",
    "    Only performs `valid` convolution.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, unit_ksize, unit_ch, unit_count, split=1):\n",
    "        super().__init__()\n",
    "        assert len(unit_ksize) == len(unit_ch), \"Unbalance Unit Info\"\n",
    "\n",
    "        self.nr_unit = unit_count\n",
    "        self.in_ch = in_ch\n",
    "        self.unit_ch = unit_ch\n",
    "\n",
    "        # ! For inference only so init values for batchnorm may not match tensorflow\n",
    "        unit_in_ch = in_ch\n",
    "        pad_vals = [v // 2 for v in unit_ksize]\n",
    "        self.units = nn.ModuleList()\n",
    "        for idx in range(unit_count):\n",
    "            self.units.append(\n",
    "                nn.Sequential(\n",
    "                    nn.BatchNorm2d(unit_in_ch, eps=1e-5),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(\n",
    "                        unit_in_ch, unit_ch[0], unit_ksize[0],\n",
    "                        stride=1, padding=pad_vals[0], bias=False,\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(unit_ch[0], eps=1e-5),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(\n",
    "                        unit_ch[0], unit_ch[1], unit_ksize[1],\n",
    "                        stride=1, padding=pad_vals[1], bias=False,\n",
    "                        groups=split,\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "            unit_in_ch += unit_ch[1]\n",
    "\n",
    "        self.blk_bna = nn.Sequential(\n",
    "            nn.BatchNorm2d(unit_in_ch, eps=1e-5),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def out_ch(self):\n",
    "        return self.in_ch + self.nr_unit * self.unit_ch[-1]\n",
    "\n",
    "    def forward(self, prev_feat):\n",
    "        for idx in range(self.nr_unit):\n",
    "            new_feat = self.units[idx](prev_feat)\n",
    "            prev_feat = torch.cat([prev_feat, new_feat], dim=1)\n",
    "        prev_feat = self.blk_bna(prev_feat)\n",
    "\n",
    "        return prev_feat\n",
    "\n",
    "\n",
    "class HoVerNetConic(ModelABC):\n",
    "    \"\"\"Initialise HoVer-Net.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_types=None,\n",
    "            freeze=False,\n",
    "            pretrained_backbone=None,\n",
    "            ):\n",
    "        super().__init__()\n",
    "        self.freeze = freeze\n",
    "        self.num_types = num_types\n",
    "        self.output_ch = 3 if num_types is None else 4\n",
    "\n",
    "        self.backbone = ResNetExt.resnet50(\n",
    "            3, pretrained=pretrained_backbone)\n",
    "        self.conv_bot = nn.Conv2d(\n",
    "            2048, 1024, 1, stride=1, padding=0, bias=False)\n",
    "\n",
    "        def create_decoder_branch(out_ch=2, ksize=5):\n",
    "            pad = ksize // 2\n",
    "            module_list = [\n",
    "                nn.Conv2d(1024, 256, ksize, stride=1, padding=pad, bias=False),\n",
    "                DenseBlock(256, [1, ksize], [128, 32], 8, split=4),\n",
    "                nn.Conv2d(512, 512, 1, stride=1, padding=0, bias=False),\n",
    "            ]\n",
    "            u3 = nn.Sequential(*module_list)\n",
    "\n",
    "            module_list = [\n",
    "                nn.Conv2d(512, 128, ksize, stride=1, padding=pad, bias=False),\n",
    "                DenseBlock(128, [1, ksize], [128, 32], 4, split=4),\n",
    "                nn.Conv2d(256, 256, 1, stride=1, padding=0, bias=False),\n",
    "            ]\n",
    "            u2 = nn.Sequential(*module_list)\n",
    "\n",
    "            module_list = [\n",
    "                nn.Conv2d(256, 64, ksize, stride=1, padding=pad, bias=False),\n",
    "            ]\n",
    "            u1 = nn.Sequential(*module_list)\n",
    "\n",
    "            module_list = [\n",
    "                nn.BatchNorm2d(64, eps=1e-5),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(64, out_ch, 1, stride=1, padding=0, bias=True),\n",
    "            ]\n",
    "            u0 = nn.Sequential(*module_list)\n",
    "\n",
    "            decoder = nn.Sequential(\n",
    "                OrderedDict([(\"u3\", u3), (\"u2\", u2), (\"u1\", u1), (\"u0\", u0)])\n",
    "            )\n",
    "            return decoder\n",
    "\n",
    "        ksize = 3\n",
    "        if num_types is None:\n",
    "            self.decoder = nn.ModuleDict(\n",
    "                OrderedDict(\n",
    "                    [\n",
    "                        (\"np\", create_decoder_branch(ksize=ksize, out_ch=2)),\n",
    "                        (\"hv\", create_decoder_branch(ksize=ksize, out_ch=2)),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            self.decoder = nn.ModuleDict(\n",
    "                OrderedDict(\n",
    "                    [\n",
    "                        (\"tp\", create_decoder_branch(ksize=ksize, out_ch=num_types)),\n",
    "                        (\"np\", create_decoder_branch(ksize=ksize, out_ch=2)),\n",
    "                        (\"hv\", create_decoder_branch(ksize=ksize, out_ch=2)),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.upsample2x = UpSample2x()\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        imgs = imgs / 255.0  # to 0-1 range to match XY\n",
    "\n",
    "        d0, d1, d2, d3 = self.backbone(imgs, self.freeze)\n",
    "        d3 = self.conv_bot(d3)\n",
    "        d = [d0, d1, d2, d3]\n",
    "\n",
    "        out_dict = OrderedDict()\n",
    "        for branch_name, branch_desc in self.decoder.items():\n",
    "            u3 = self.upsample2x(d[-1]) + d[-2]\n",
    "            u3 = branch_desc[0](u3)\n",
    "\n",
    "            u2 = self.upsample2x(u3) + d[-3]\n",
    "            u2 = branch_desc[1](u2)\n",
    "\n",
    "            u1 = self.upsample2x(u2) + d[-4]\n",
    "            u1 = branch_desc[2](u1)\n",
    "\n",
    "            u0 = branch_desc[3](u1)\n",
    "            out_dict[branch_name] = u0\n",
    "\n",
    "        return out_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def _proc_np_hv(np_map: np.ndarray, hv_map: np.ndarray, fx: float = 1):\n",
    "        return TIAHoVerNet._proc_np_hv(np_map, hv_map, fx)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_instance_info(pred_inst, pred_type=None):\n",
    "        return TIAHoVerNet._get_instance_info(pred_inst, pred_type)\n",
    "\n",
    "    @staticmethod\n",
    "    # skipcq: PYL-W0221\n",
    "    def postproc(raw_maps: List[np.ndarray]):\n",
    "        return TIAHoVerNet.postproc(raw_maps)\n",
    "\n",
    "    @staticmethod\n",
    "    def infer_batch(model, batch_data, on_gpu):\n",
    "        return TIAHoVerNet.infer_batch(model, batch_data, on_gpu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import joblib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.utils import io as IPyIO\n",
    "from tqdm import tqdm\n",
    "\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "\n",
    "# adding the project root folder\n",
    "sys.path.append('../')\n",
    "from tiatoolbox.models import IOSegmentorConfig, SemanticSegmentor\n",
    "from tiatoolbox.utils.visualization import overlay_prediction_contours\n",
    "\n",
    "# from ../input/hovernet-conic-weights/misc.utils import cropping_center, recur_find_ext, rm_n_mkdir, rmdir\n",
    "\n",
    "# Random seed for deterministic\n",
    "SEED = 5\n",
    "# The number of nuclei within the dataset/predictions.\n",
    "# For CoNIC, we have 6 (+1 for background) types in total.\n",
    "NUM_TYPES = 7\n",
    "# The path to the directory containg images.npy etc.\n",
    "DATA_DIR = '../data/'\n",
    "# The path to the pretrained weights\n",
    "PRETRAINED = '../pretrained/hovernet-conic.pth'\n",
    "# The path to contain output and intermediate processing results\n",
    "OUT_DIR = './exp_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Generating Data Splits\n",
    " Here, we show how the training data is split into the training\n",
    " and validation subsets. The CoNIC training data comes from multiple\n",
    " sources. To ensure that we have a balanced dataset, we use stratified sampling according to the data source.\n",
    "\n",
    " For the baseline model, we utilize 80% the number of patches\n",
    " for training and the remaining for validation. However, because\n",
    " we apply stratified sampling according to the origin, the final number of patches may not be 80/20 (each image may contain a different number of patches). Therefore, we generate a number of splits (indicated\n",
    " via the `NUM_TRIALS` variable) and select the one that has the\n",
    " number of patches that most closely matches with our expected ratio.\n",
    "\n",
    " Lastly, to ensure the reproducibility of the sampling, we also provide a random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_TRIALS = 10  # number of splits to be generated\n",
    "TRAIN_SIZE = 0.8\n",
    "VALID_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 3963 - Valid: 1018\n",
      "Train: 4053 - Valid: 0928\n",
      "Train: 3952 - Valid: 1029\n",
      "Train: 3988 - Valid: 0993\n",
      "Train: 3997 - Valid: 0984\n",
      "Train: 4002 - Valid: 0979\n",
      "Train: 3894 - Valid: 1087\n",
      "Train: 4012 - Valid: 0969\n",
      "Train: 3988 - Valid: 0993\n",
      "Train: 3964 - Valid: 1017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../data//splits.dat']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "info = pd.read_csv(f'{DATA_DIR}/patch_info.csv')\n",
    "file_names = np.squeeze(info.to_numpy()).tolist()\n",
    "\n",
    "img_sources = [v.split('-')[0] for v in file_names]\n",
    "img_sources = np.unique(img_sources)\n",
    "\n",
    "cohort_sources = [v.split('_')[0] for v in img_sources]\n",
    "_, cohort_sources = np.unique(cohort_sources, return_inverse=True)\n",
    "\n",
    "splitter = StratifiedShuffleSplit(\n",
    "    n_splits=NUM_TRIALS,\n",
    "    train_size=TRAIN_SIZE,\n",
    "    test_size=VALID_SIZE,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "splits = []\n",
    "split_generator = splitter.split(img_sources, cohort_sources)\n",
    "for train_indices, valid_indices in split_generator:\n",
    "    train_cohorts = img_sources[train_indices]\n",
    "    valid_cohorts = img_sources[valid_indices]\n",
    "    assert np.intersect1d(train_cohorts, valid_cohorts).size == 0\n",
    "    train_names = [\n",
    "        file_name\n",
    "        for file_name in file_names\n",
    "        for source in train_cohorts\n",
    "        if source == file_name.split('-')[0]\n",
    "    ]\n",
    "    valid_names = [\n",
    "        file_name\n",
    "        for file_name in file_names\n",
    "        for source in valid_cohorts\n",
    "        if source == file_name.split('-')[0]\n",
    "    ]\n",
    "    train_names = np.unique(train_names)\n",
    "    valid_names = np.unique(valid_names)\n",
    "    print(f'Train: {len(train_names):04d} - Valid: {len(valid_names):04d}')\n",
    "    assert np.intersect1d(train_names, valid_names).size == 0\n",
    "    train_indices = [file_names.index(v) for v in train_names]\n",
    "    valid_indices = [file_names.index(v) for v in valid_names]\n",
    "    splits.append({\n",
    "        'train': train_indices,\n",
    "        'valid': valid_indices\n",
    "    })\n",
    "joblib.dump(splits, f\"{DATA_DIR}/splits.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Inference\n",
    " To further simplify the inference process, we utilise `tiatoolbox`\n",
    " which already contains the inference mechanism. This functionality\n",
    " accepts a list of image paths as input. Therefore, we will turn `images.npy` into individual `*.png` for the inference process.\n",
    "\n",
    " > **Note**: We use the first fold (trial) from the splits generated above\n",
    " > for training. Therefore, we need to extract the corresponding ground truth\n",
    " > for the validation evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The fold to use\n",
    "FOLD_IDX = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imgs = np.load(f'{DATA_DIR}/images.npy')\n",
    "labels = np.load(f'{DATA_DIR}/labels.npy')\n",
    "\n",
    "splits = joblib.load(f'{DATA_DIR}/splits.dat')\n",
    "valid_indices = splits[FOLD_IDX]['valid']\n",
    "\n",
    "rm_n_mkdir(f'{OUT_DIR}/imgs/')\n",
    "for idx in valid_indices:\n",
    "    img = imgs[idx]\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(f'{OUT_DIR}/imgs/{idx:04d}.png', img)\n",
    "\n",
    "valid_labels = labels[valid_indices]\n",
    "np.save(f'{OUT_DIR}/valid_true.npy', valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from net_desc import HoVerNetConic\n",
    "pretrained = torch.load(PRETRAINED, map_location=torch.device('cpu'))\n",
    "model = HoVerNetConic(num_types=NUM_TYPES)\n",
    "model.load_state_dict(pretrained)\n",
    "\n",
    "# Tile prediction\n",
    "predictor = SemanticSegmentor(\n",
    "    model=model,\n",
    "    num_loader_workers=2,\n",
    "    batch_size=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|2022-08-30|15:15:50.261| [WARNING] C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tiatoolbox\\models\\engine\\semantic_segmentor.py:1077: UserWarning: WSIPatchDataset only reads image tile at `units=\"baseline\"`. Resolutions will be converted to baseline value.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# capture all the printing to avoid cluttering the console\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m IPyIO\u001b[38;5;241m.\u001b[39mcapture_output() \u001b[38;5;28;01mas\u001b[39;00m captured:\n\u001b[1;32m---> 25\u001b[0m     output_file \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_img_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtile\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_gpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mioconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcrash_on_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mOUT_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/raw/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tiatoolbox\\models\\engine\\semantic_segmentor.py:1289\u001b[0m, in \u001b[0;36mSemanticSegmentor.predict\u001b[1;34m(self, imgs, masks, mode, on_gpu, ioconfig, patch_input_shape, patch_output_shape, stride_shape, resolution, units, save_dir, crash_on_exception)\u001b[0m\n\u001b[0;32m   1287\u001b[0m \u001b[38;5;66;03m# use external for testing\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_gpu \u001b[38;5;241m=\u001b[39m on_gpu\n\u001b[1;32m-> 1289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[43mmisc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_to\u001b[49m\u001b[43m(\u001b[49m\u001b[43mon_gpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;66;03m# workers should be > 0 else Value Error will be thrown\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_workers()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tiatoolbox\\utils\\misc.py:762\u001b[0m, in \u001b[0;36mmodel_to\u001b[1;34m(on_gpu, model)\u001b[0m\n\u001b[0;32m    760\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_gpu:  \u001b[38;5;66;03m# DataParallel work only for cuda\u001b[39;00m\n\u001b[0;32m    761\u001b[0m     model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mDataParallel(model)\n\u001b[1;32m--> 762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    923\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    924\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m    925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m--> 927\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 602\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    603\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m    923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    924\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\cuda\\__init__.py:211\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# Define the input/output configurations\n",
    "ioconfig = IOSegmentorConfig(\n",
    "    input_resolutions=[\n",
    "        {'units': 'baseline', 'resolution': 1.0},\n",
    "    ],\n",
    "    output_resolutions=[\n",
    "        {'units': 'baseline', 'resolution': 1.0},\n",
    "        {'units': 'baseline', 'resolution': 1.0},\n",
    "        {'units': 'baseline', 'resolution': 1.0},\n",
    "    ],\n",
    "    save_resolution={'units': 'baseline', 'resolution': 1.0},\n",
    "    patch_input_shape=[256, 256],\n",
    "    patch_output_shape=[256, 256],\n",
    "    stride_shape=[256, 256],\n",
    ")\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.disabled = True\n",
    "\n",
    "infer_img_paths = recur_find_ext(f'{OUT_DIR}/imgs/', ['.png'])\n",
    "rmdir(f'{OUT_DIR}/raw/')\n",
    "\n",
    "# capture all the printing to avoid cluttering the console\n",
    "with IPyIO.capture_output() as captured:\n",
    "    output_file = predictor.predict(\n",
    "        infer_img_paths,\n",
    "        masks=None,\n",
    "        mode='tile',\n",
    "        on_gpu=True,\n",
    "        ioconfig=ioconfig,\n",
    "        crash_on_exception=True,\n",
    "        save_dir=f'{OUT_DIR}/raw/'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Once we have the HoVerNet raw inference output, we apply post processing\n",
    " to obtain the final results. For CoNIC, there are two tasks that\n",
    " are linked with each other:\n",
    " - The instance segmentation and classification of nuclei.\n",
    " - The cellular compositions within the provided patches.\n",
    " Rather than directly predicting the 2nd set of results from images, we simply use the final output of HoVerNet. Thus, to make the code more organised, we separate these tasks into their own funtion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_segmentation(np_map, hv_map, tp_map):\n",
    "    # HoVerNet post-proc is coded at 0.25mpp so we resize\n",
    "    np_map = cv2.resize(np_map, (0, 0), fx=2.0, fy=2.0)\n",
    "    hv_map = cv2.resize(hv_map, (0, 0), fx=2.0, fy=2.0)\n",
    "    tp_map = cv2.resize(\n",
    "                    tp_map, (0, 0), fx=2.0, fy=2.0,\n",
    "                    interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    inst_map = model._proc_np_hv(np_map[..., None], hv_map)\n",
    "    inst_dict = model._get_instance_info(inst_map, tp_map)\n",
    "\n",
    "    # Generating results match with the evaluation protocol\n",
    "    type_map = np.zeros_like(inst_map)\n",
    "    inst_type_colours = np.array([\n",
    "        [v['type']] * 3 for v in inst_dict.values()\n",
    "    ])\n",
    "    type_map = overlay_prediction_contours(\n",
    "        type_map, inst_dict,\n",
    "        line_thickness=-1,\n",
    "        inst_colours=inst_type_colours)\n",
    "\n",
    "    pred_map = np.dstack([inst_map, type_map])\n",
    "    # The result for evaluation is at 0.5mpp so we scale back\n",
    "    pred_map = cv2.resize(\n",
    "                    pred_map, (0, 0), fx=0.5, fy=0.5,\n",
    "                    interpolation=cv2.INTER_NEAREST)\n",
    "    return pred_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_composition(pred_map):\n",
    "    # Only consider the central 224x224 region,\n",
    "    # as noted in the challenge description paper\n",
    "    pred_map = cropping_center(pred_map, [224, 224])\n",
    "    inst_map = pred_map[..., 0]\n",
    "    type_map = pred_map[..., 1]\n",
    "    # ignore 0-th index as it is 0 i.e background\n",
    "    uid_list = np.unique(inst_map)[1:]\n",
    "\n",
    "    if len(uid_list) < 1:\n",
    "        type_freqs = np.zeros(NUM_TYPES)\n",
    "        return type_freqs\n",
    "    uid_types = [\n",
    "        np.unique(type_map[inst_map == uid])\n",
    "        for uid in uid_list\n",
    "    ]\n",
    "    type_freqs_ = np.unique(uid_types, return_counts=True)\n",
    "    # ! not all types exist within the same spatial location\n",
    "    # ! so we have to create a placeholder and put them there\n",
    "    type_freqs = np.zeros(NUM_TYPES)\n",
    "    type_freqs[type_freqs_[0]] = type_freqs_[1]\n",
    "    return type_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we actually perform the post-processing using the input\n",
    " output file mapping obtained previously from the inference process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_file = f'{OUT_DIR}/raw/file_map.dat'\n",
    "output_info = joblib.load(output_file)\n",
    "\n",
    "semantic_predictions = []\n",
    "composition_predictions = []\n",
    "for input_file, output_root in tqdm(output_info):\n",
    "    img = cv2.imread(input_file)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    np_map = np.load(f'{output_root}.raw.0.npy')\n",
    "    hv_map = np.load(f'{output_root}.raw.1.npy')\n",
    "    tp_map = np.load(f'{output_root}.raw.2.npy')\n",
    "\n",
    "    pred_map = process_segmentation(np_map, hv_map, tp_map)\n",
    "    type_freqs = process_composition(pred_map)\n",
    "    semantic_predictions.append(pred_map)\n",
    "    composition_predictions.append(type_freqs)\n",
    "semantic_predictions = np.array(semantic_predictions)\n",
    "composition_predictions = np.array(composition_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving the results for segmentation\n",
    "np.save(f'{OUT_DIR}/valid_pred.npy', semantic_predictions)\n",
    "\n",
    "# Saving the results for composition prediction\n",
    "TYPE_NAMES = [\n",
    "    \"neutrophil\", \"epithelial\", \"lymphocyte\",\n",
    "    \"plasma\", \"eosinophil\", \"connective\"\n",
    "]\n",
    "df = pd.DataFrame(\n",
    "    composition_predictions[:, 1:].astype(np.int32),\n",
    ")\n",
    "df.columns = TYPE_NAMES\n",
    "df.to_csv(f'{OUT_DIR}/valid_pred_cell.csv', index=False)\n",
    "\n",
    "# Load up the composition ground truth and\n",
    "# save the validation portion\n",
    "df = pd.read_csv(f'{DATA_DIR}/counts.csv')\n",
    "true_compositions = df.to_numpy()[valid_indices]\n",
    "df = pd.DataFrame(\n",
    "    true_compositions.astype(np.int32),\n",
    ")\n",
    "df.columns = TYPE_NAMES\n",
    "df.to_csv(f'{OUT_DIR}/valid_true_cell.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Visualization\n",
    " To wrap everything up, we randomly select some samples within the validation\n",
    " set and plot their associated ground truth and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './exp_output/raw/file_map.dat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m semantic_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/valid_pred.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/raw/file_map.dat\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 5\u001b[0m output_info \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(SEED)\n\u001b[0;32m      8\u001b[0m selected_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mlen\u001b[39m(valid_indices), \u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\Conic_Challenge\\lib\\site-packages\\joblib\\numpy_pickle.py:579\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    577\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 579\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[0;32m    581\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    582\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    583\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    584\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './exp_output/raw/file_map.dat'"
     ]
    }
   ],
   "source": [
    "semantic_true = np.load(f'{OUT_DIR}/valid_true.npy')\n",
    "semantic_pred = np.load(f'{OUT_DIR}/valid_pred.npy')\n",
    "\n",
    "output_file = f'{OUT_DIR}/raw/file_map.dat'\n",
    "output_info = joblib.load(output_file)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "selected_indices = np.random.choice(len(valid_indices), 4)\n",
    "\n",
    "PERCEPTIVE_COLORS = [\n",
    "    (  0,   0,   0),\n",
    "    (255, 165,   0),\n",
    "    (  0, 255,   0),\n",
    "    (255,   0,   0),\n",
    "    (  0, 255, 255),\n",
    "    (  0,   0, 255),\n",
    "    (255, 255,   0),\n",
    "]\n",
    "\n",
    "for idx in selected_indices:\n",
    "    img = cv2.imread(output_info[idx][0])\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    inst_map = semantic_pred[idx][..., 0]\n",
    "    type_map = semantic_pred[idx][..., 1]\n",
    "    pred_inst_dict = model._get_instance_info(inst_map, type_map)\n",
    "\n",
    "    inst_map = semantic_true[idx][..., 0]\n",
    "    type_map = semantic_true[idx][..., 1]\n",
    "    true_inst_dict = model._get_instance_info(inst_map, type_map)\n",
    "\n",
    "    inst_type_colours = np.array([\n",
    "        PERCEPTIVE_COLORS[v['type']]\n",
    "        for v in true_inst_dict.values()\n",
    "    ])\n",
    "    overlaid_true = overlay_prediction_contours(\n",
    "        img, true_inst_dict,\n",
    "        inst_colours=inst_type_colours,\n",
    "        line_thickness=1\n",
    "    )\n",
    "\n",
    "    inst_type_colours = np.array([\n",
    "        PERCEPTIVE_COLORS[v['type']]\n",
    "        for v in pred_inst_dict.values()\n",
    "    ])\n",
    "    overlaid_pred = overlay_prediction_contours(\n",
    "        img, pred_inst_dict,\n",
    "        inst_colours=inst_type_colours,\n",
    "        line_thickness=1\n",
    "    )\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(overlaid_true)\n",
    "    plt.title('Ground Truth')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(overlaid_pred)\n",
    "    plt.title('Prediction')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
